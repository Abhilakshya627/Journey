{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Breakdown Process\n",
    "1. Corpus/Paragraph/Sequenceof lines\n",
    "2. Sentence Tokenization\n",
    "3. Word tokenization \n",
    "4. Lower Case\n",
    "5. Remove Duplicates\n",
    "6. Remove Punctuation and Numbers\n",
    "7. Stopwords\n",
    "8. Stemming and Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my corpus is 71\n"
     ]
    }
   ],
   "source": [
    "text=\"I am student of Graphic Era (Deemed to be University). My Roll No. is 3\"\n",
    "m=len(text)\n",
    "print(f\"length of my corpus is {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenization (Corpus -> Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a student of Graphic Era (Deemed to be University).', 'My Roll No.', 'is 3.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization (Corpus->Sentence)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I am a student of Graphic Era (Deemed to be University). My Roll No. is 3.\"\n",
    "sentence = sent_tokenize(text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\user/nltk_data', 'c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data', 'c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data', 'c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenization (Corpus -> Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'student', 'of', 'Graphic', 'Era', '(', 'Deemed', 'to', 'be', 'University', ')', '.', 'My', 'Roll', 'No', '.', 'is', '3', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization (Corpus->Word)\n",
    "from nltk.tokenize import word_tokenize\n",
    "word=word_tokenize(text)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenization (Sentence->Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'student', 'of', 'Graphic', 'Era', '(', 'Deemed', 'to', 'be', 'University', ')', '.']\n",
      "['My', 'Roll', 'No', '.']\n",
      "['is', '3', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization (Sentence->Word)\n",
    "for i in sentence:\n",
    "    w=word_tokenize(i)\n",
    "    print(w)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'a', 'student', 'of', 'graphic', 'era', '(', 'deemed', 'to', 'be', 'university', ')', '.', 'my', 'roll', 'no', '.', 'is', '3', '.']\n"
     ]
    }
   ],
   "source": [
    "#Lower Case\n",
    "word=list(map(str.lower,word))\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'to', 'am', 'i', ')', 'be', '(', 'no', 'a', '3', 'is', 'student', 'deemed', 'of', '.', 'roll', 'university', 'era', 'graphic']\n"
     ]
    }
   ],
   "source": [
    "word=list(set(word))\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'to', 'am', 'i', 'be', 'no', 'a', 'is', 'student', 'deemed', 'of', 'roll', 'university', 'era', 'graphic']\n"
     ]
    }
   ],
   "source": [
    "alpha_words=[]\n",
    "for i in word:\n",
    "    if i.isalpha():\n",
    "        alpha_words.append(i)\n",
    "print(alpha_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")[:10] #[:10] shows starting 10 words only'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['student', 'deemed', 'roll', 'university', 'era', 'graphic']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words=[]\n",
    "for word in alpha_words:\n",
    "    if word not in stopwords.words(\"english\"):\n",
    "        filtered_words.append(word)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word : student \tAfter Stemming : student\n",
      "Original Word : deemed \tAfter Stemming : deem\n",
      "Original Word : roll \tAfter Stemming : roll\n",
      "Original Word : university \tAfter Stemming : univers\n",
      "Original Word : era \tAfter Stemming : era\n",
      "Original Word : graphic \tAfter Stemming : graphic\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "for i in filtered_words:\n",
    "    print(f\"Original Word : {i} \\tAfter Stemming : {ps.stem(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student', 'deemed', 'roll', 'university', 'era', 'graphic']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "'''import nltk\n",
    "nltk.download('wordnet')'''\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "clean_text=[]\n",
    "for i in filtered_words:\n",
    "    clean_text.append(wnl.lemmatize(i))\n",
    "print(clean_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
